---
title: "Comparing Jungle to Drum & Bass music"
author: "Skip Vial"
date: "March 2021"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: rows
---

```{r setup, include=FALSE}

library(tidyverse)
library(spotifyr)
library(usethis)
library(ggthemes)
library(compmus)
library(remotes)
library(plotly)
library(viridis)
library(hrbrthemes)
library(plyr)
library(tidymodels)
library(heatmaply)
library(ggdendro)
library(kknn)
library(C50)
library(ranger)
```

```{r templates}

### Import Spotify Playlists ###
Jungle <- get_playlist_audio_features("", "4Xq7CHKa693iezbOXH8NOQ")

RaggaDnB <- get_playlist_audio_features("", "0ix8qCnYCiu4w8OcnK0Uqu")

LightDnB <- get_playlist_audio_features("", "5ABMzUESx7K7EyowE5kFCl")

Heavy1 <- get_playlist_audio_features("", "7Dx1mjAUeV8ElB5FERajkl")
Heavy2 <- get_playlist_audio_features("", "0Po1Xhn50bsrQwyjXWMMKJ")


### Combine datasets per subgenre ###
HeavyDnB <-
  bind_rows(
    Heavy1 %>% mutate(category = "Heavy1"),
    Heavy2 %>% mutate(category = "Heavy2")
  )


### Final final universal dataframe ###
all_music <-
  bind_rows(
      Jungle %>% mutate(category = "Jungle"),
      RaggaDnB %>% mutate(category = "RaggaDnB"),
      LightDnB %>% mutate(category = "LightDnB"),
      HeavyDnB %>% mutate(category = "HeavyDnB")
  )

### Delete useless columns ###
all_music = subset(all_music, select = -c(playlist_id, playlist_img, playlist_owner_name,
                                    playlist_owner_id, track.id, analysis_url,
                                    is_local, primary_color, added_by.href,
                                    added_by.id, added_by.type, added_by.uri,
                                    added_by.external_urls.spotify, track.artists,
                                    track.available_markets, track.disc_number,
                                    track.episode, track.explicit, track.href,
                                    track.is_local, track.preview_url, track.track,
                                    track.track_number, track.type, track.uri,
                                    track.album.album_type, track.album.artists,
                                    track.album.available_markets, track.album.href,
                                    track.album.id, track.album.images, 
                                    track.album.name, added_at,
                                    track.album.release_date_precision,
                                    track.album.total_tracks, track.album.type,
                                    track.album.uri, track.album.external_urls.spotify,
                                    track.external_ids.isrc, 
                                    track.external_urls.spotify, video_thumbnail.url
                                    ))

dnb <-
  bind_rows(
      Jungle %>% mutate(playlist = "Jungle") %>% slice_head(n = 25),
      RaggaDnB %>% mutate(playlist = "RaggaDnB") %>% slice_head(n = 25),
      LightDnB %>% mutate(playlist = "LightDnB") %>% slice_head(n = 25),
      HeavyDnB %>% mutate(playlist = "HeavyDnB") %>% slice_head(n = 25)
  )


# Make the key profiles
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

```



### NEW! Which **features** are important in **distinguishing the genres**?

```{r out.width="50%"}

# Make the functions
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
}

# Summarise the vectors
dnb_features <-
  dnb %>%  # For your portfolio, change this to the name of your corpus.
  add_audio_analysis() %>% 
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))

# Choosing useful variables for predicting 
dnb_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      speechiness +
      acousticness +
      instrumentalness +
      valence +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = dnb_features,          # Use the same name as the previous block.
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].

# Set up cross validation
dnb_cv <- dnb_features %>% vfold_cv(5)

# k-model
knn_model <-
  nearest_neighbor(neighbors = 1) %>%
  set_mode("classification") %>% 
  set_engine("kknn")
dnb_knn <- 
  workflow() %>% 
  add_recipe(dnb_recipe) %>% 
  add_model(knn_model) %>% 
  fit_resamples(
    dnb_cv, 
    control = control_resamples(save_pred = TRUE)
  )

# Figure showing distribution
dnb_knn %>% get_conf_mat() %>% autoplot(type = "mosaic") +
  labs(
    title = "Predicting the playlist of each track"
  ) +
  theme(plot.title = element_text(face = "bold",
                                   size = 14)) 

# second plot
dnb_features %>%
  ggplot(aes(x = valence, y = c08, colour = playlist, size = instrumentalness)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    x = "Valence",
    y = "Timbre Component 8",
    size = "Instrumentalness",
    colour = "Playlist",
    title = "Most important feature differences"
  ) +
  theme_light() +
  theme(plot.title = element_text(face = "bold",
                                   size = 14)) 

```

***

***Mosaic plot (left)***

It is interesting to investigate if an **algorithm** would classify each track and predict in which of the four Drum & Bass playlists the track belongs. If the algorithm is mainly correct, it means that the playlists significantly differ from each other. For the computer to be able to handle the computation, only the **top 25 tracks of each playlist were compared** to each other. On the other hand, including more tracks would increase content validity, but I believe that a quarter of each playlist is sufficient to gain significant results.

The results of the algorithm are presented in the plot on the left. The length of a 'block' shows the amount of tracks which the algorithm thinks it belongs to the genre. If the genre of the largest block matches the genre of the 'truth' on the bottom, the algorithm did well in predicting the genre. We can see that the algorithm did moderately well for HeavyDnB. Around 50% of the tracks were correctly predicted as HeavyDnB. **The algorithm performed the best for LightDnB**. The overwhelming majority of LightDnB tracks was identified as a LightDnB track.

**The algorithm was not able to distinguish Jungle from RaggaDnB**. We can see for both genres that there is an almost even divide. So apparently, Jungle and RaggaDnB do not differ that much from each other.

***Main feature differences***

So, how can this be explained? In the right plot, we look at the three most important features with which the algorithm determined in which genre the tracks belong. First, we can see clearly that **LightDnB separates itself with Timbre Component 8** (around the value 0). It's great that there is such a clear divide, but unfortunately we will never know what Timbre Component 8 exactly entails. The three remaining genres all mainly have a Timbre Component 8 of zero or higher. 

Next, **HeavyDnB is separated by instrumentalness and valence**. HeavyDnB tracks are mostly placed at the left of the plot, which indicates a general low valence. Additionally, where Jungle and RaggaDnB have low instrumentalness (smaller dots), HeavyDnB is very instrumental (bigger dots). 

Lastly, it becomes evident from this plot that **Jungle and RaggaDnB cannot be distinguished from each other** by the most important features. When looking at the plot, you can see that LightDnB and HeavyDnB seem to have their own space, but it is very hard to say the same for Jungle and RaggaDnB since there is significant overlap between the two.



### Welcome to my portfolio!

*Background*

The music genre ‘Jungle’ emerged in the 1990s and is viewed as the direct originating point for the newer music genre named ‘Drum & Bass’ (emerged mid 1990s). These two genres are commonly used as synonyms for one another. Almost 30 years have passed since these genres were developed. During this time the Drum & Bass genre has grown significantly in terms of exposure and now knows many subgenres, while Jungle music did not make the same growth and lost popularity. What caused this turn of events? Has Drum & Bass become more popular because of its diversity? Can we still use the terms ‘Jungle’ and ‘Drum & Bass’ as synonyms or has Drum & Bass developed in such a way that it has become completely different from its originating point? This corpus analysis attempts to answer these questions by comparing Jungle music to various subgenres of Drum & Bass music:

- Ragga Drum & Bass. This subgenre was inspired by the original Ragga Jungle style, which was very popular at the time. Expected is that this genre is most similar to the original Jungle music.

- Light Drum & Bass. Also called Liquid Drum & Bass. Many harmonic and melodic grooves are used, as well as samples from funk, jazz, soul, R&B. Expected is that this subgenre has significant differences from Jungle. Overall, Liquid Drum & Bass is known for its positive energy. This would mean that the acousticness and/or valence of songs are different than in Jungle music.

- Heavy Drum & Bass. This subgenre has a general ‘dark’ mood, which is realized with deeper basslines and more industrial, hardcore (electronic) melodies.

*** 

*Corpus*

This corpus uses genre-based Spotify Playlists.

- Jungle:
  - JUNGLE / RAGGA DNB / JUNGLE REVIVAL
  - Tracks: 125
  
- Ragga DnB: 
  - Ragga Drum and Bass
  - Tracks: 115
  
- Light DnB:
  - Liquid Drum and Bass 	
  - Tracks: 100
  
- Heavy DnB:
  - Neurofunk Drum & Bass
  - NEUROFUNK 2021 // Updated Weekly
  - Tracks: 113



###  First, we investigate **how much vocals** are used in each genre

```{r }

# Violin plot instrumentalness per genre
instrumentalness <- all_music %>%
  ggplot( aes(x=instrumentalness, 
              y=category, 
              fill=category, 
              color=category,)) +
    geom_violin(width=1.6, size=0.5) +
    labs(x = "Instrumentalness",
         title = "",
         subtitle = "The distribution of instrumentalness per genre",
         caption = ""
         ) + 
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    theme_ipsum() +
    theme(
      legend.position="none",
      axis.title.y = element_blank(),
      plot.subtitle = element_text(face = "bold",
                                   size = 14),
      axis.title.x = element_text(size = 10)
    ) 

instrumentalness
```

***

Instrumentalness predicts whether a track contains vocals or not. A value of 0.5 or higher represents a track that contains no vocal content. In this plot, **the thickness of the line represents the amount of tracks** that have the same instrumentalness value. This plot shows a pattern between the genres that was expected. The genres **Jungle and RaggaDnB** are very similar and both contain **mostly vocal tracks** since most tracks have an instrumentalness value between 0 and 0.25.  

LightDnB and HeavyDnB also show a somewhat similar distribution. The differences between these genres is that **HeavyDnB contains mostly non-vocal tracks**, where **LightDnB** shows an almost **equal amount of vocal and non-vocal tracks**.



### Second, Is it possible that the **presence of vocals influences other features**? 

```{r out.width="50%" }

# Scatterplot energy, valence and danceability
energy_valence <- ggplot(all_music, 
       aes(energy, 
           valence, 
           color = danceability)) + 
  geom_point(position = "jitter", alpha = 0.7, shape = 20) +
      scale_fill_viridis(discrete=FALSE) +
    scale_color_viridis(discrete=FALSE) +
  theme_light() +
  facet_wrap(~ category) +
    labs(     
    x = "Energy",
    y = "Valence",
    color = "Danceability",
    title = "Energy, valence and danceability per genre",
    caption = "") +
      theme(plot.title = element_text(face = "bold",
                                   size = 14),
      axis.title.x = element_text(size = 10),
      axis.title.y = element_text(size = 10)
    ) 

energy_valence


```

***

***Energy***

*The energy value ranges from 0.0 to 1.0. A higher value means that the track feels fast, loud and noisy.*

It becomes clear that the genre HeavyDnB consists mostly of tracks with very high energy. This was expected since this genre draws influence from hardcore music, for example. The genres Jungle and RaggaDnB also show many tracks with high energy, possibly because there always is a consistent drum pattern in addition to vocals. LightDnB tracks show more variety in the amount of energy. This is not surprising as this genre is usually has a 'laid-back' feel to it.

***Valence***

*Valence describes how positive a track feels. A high valence corresponds to a more happy or euphoric track. Tracks with low valence sound more sad, depressed or angry.*

As for valence, the genres can be divided into two groups. HeavyDnB and LightDnB show a tendency for tracks with low valence, whereas Jungle and RaggaDnB consist of tracks with a general high valence. In the first plot we could see the same split between these genres. Is it possible that the presence of vocals influences this other features?

*Note: it is surprising that LightDnB has many tracks with low valence. A general high valence was expected here, since this genre is considered as the more 'feel-good' subgenre of Drum & Bass*.

***Danceability***

*Danceability describes how suitable a track is for dancing. In this plot, a light color represents a high danceability and a dark color represents low danceability.*

The genres can be divided in the same manner as with valence. Jungle and RaggaDnB have more tracks than HeavyDnB and LightDnB that are considered 'danceable' by Spotify. 

Now why is that? The biggest difference measured between the genres is that Jungle and RaggaDnB often have vocals in tracks, whereas HeavyDnB and Light DnB have more non-vocal tracks. It is possible that there is a relationship between the presence of vocals and the features described here. Possibly, **the presence of vocals positively influences valence and/or danceability**. 


### And the **most popular** genre is...

```{r}

# Plot track popularity

popularity_means <- ddply(all_music, "category", summarise, grp.mean=mean(track.popularity))


plot_popularity <- all_music %>% ggplot(aes(x = track.popularity,
                         fill = category
                         )) +
  geom_histogram(binwidth = 1,
                 alpha = 0.7) +
  geom_vline(data = popularity_means, aes(xintercept = grp.mean, 
                                          color = category),
                                          size = 1,
                                          alpha = 0.8,
                                          linetype = "dashed") +
  labs(title = "The popularity of tracks per genre",
       x = "Popularity",
       y = "Count") +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
  theme_ipsum() +
  theme(legend.title = element_blank()
        )

  ggplotly(plot_popularity)
  


```

***

***Track Popularity***

*The popularity of a track is a value between 0 and 100, with 100 being the most popular. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past.*

**(This is an interactive plot. Hover over the plot and the top of the vertical lines to see more detailed information)**

From this plot we can see that there are clear differences in how often the genres are listened to. The **mean popularity** of each genre is represented by the vertical lines. LightDnB is most popular, followed by HeavyDnB, RaggaDnB, and finally Jungle as the least popular genre. What really catches the eye is the tall bar at popularity 0. More than 50 tracks in the Jungle playlist have recently not been listened to at all.

LightDnB might be the most popular because it is considered more 'feel good' music. Generally speaking, this type of music attrracts the most people. Also when you're new to this genre, LightDnB is the 'easiest' genre to listen to.

A limitation could be the fact that only a single playlist was chosen for each genre. For example, it is possible that there are many more Jungle tracks available that are popular. If more tracks would be included for each genre, the average popularity might vary. The external validity might not be great regarding track popularity, therefore we have to be careful in interpreting this plot.



### *Chromagram Comparison 1* - What does the general build up of a **Light Drum & Bass** track look like?

```{r out.width="50%"}

# Get track information
fall_to_you <-
  get_tidy_audio_analysis("1ObLc50qTIp8gF1Lg2GimH") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

# Make the chromagram. Use manhattan, euclidean, chebyshev
chroma_fall_to_you <- fall_to_you %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", 
       y = NULL, 
       fill = "Magnitude",
       title = "Chromagram") +
  theme_minimal() +
  scale_fill_viridis_c() +
  theme(plot.title = element_text(face = "bold",
                                   size = 14))

# Add lines and text at key moments
chroma_fall_to_you +
  geom_vline(xintercept = 89,
             color = "white",
             size = 0.8) +
  geom_vline(xintercept = 200,
             color = "white",
             size = 0.8) + 
  geom_vline(xintercept = 223,
             color = "white",
             size = 0.8) +
  geom_vline(xintercept = 356,
             color = "white",
             size = 0.8) +
  annotate(geom="text", 
          x=89, 
          y="A", 
          label="First drop",
          color="white",
          fontface = 2,
          hjust=0) +
  annotate(geom="text", 
          x=200, 
          y="B", 
          label="Breakdown",
          color="white",
          fontface=2,
          hjust=1) +
  annotate(geom="text", 
          x=223, 
          y="A", 
          label="Second drop",
          color="white",
          fontface=2,
          hjust=0) +
  annotate(geom="text", 
          x=356, 
          y="B", 
          label="Breakdown",
          color="white",
          fontface=2,
          hjust=1)



# KEYGRAM
light_keygram <-
  get_tidy_audio_analysis("1ObLc50qTIp8gF1Lg2GimH") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

light_keygram %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "",
       title = "Keygram") + 
  theme(plot.title = element_text(face = "bold",
                                   size = 14)) +
    geom_vline(xintercept = 89,
             color = "white",
             size = 0.8) +
  geom_vline(xintercept = 200,
             color = "white",
             size = 0.8) + 
  geom_vline(xintercept = 223,
             color = "white",
             size = 0.8) +
  geom_vline(xintercept = 356,
             color = "white",
             size = 0.8)


```

***

*Track: [Falls to you VIP - Calibre](https://open.spotify.com/track/1ObLc50qTIp8gF1Lg2GimH?si=5kqgrc7bQU249rXaCZ9BmQ)*

***Chromagram***

This track was chosen because it shows some aspects that usually are present in Light Drum & Bass tracks. The intro of the song lasts from the beginning until the first drop. What stands out here is that **the intro is relatively long**, it lasts approximately 90 seconds. Light Drum & Bass tracks usually have a longer intro than other genres because **the average build up is very slow**. 

The middle part of the song starts after the first drop. This is usually the part where a consistent 'Drum & Bass' drum pattern is played until the breakdown. A number of **samples are then layered** on top of each other. This creates the unique sound of the song. The samples used in this genre contains a lot of **harmonic and melodic grooves**, which makes this genre very pleasant to listen to.
This pattern of a drop followed by a breakdown is then repeated one more time. There could be small variations of the samples used, but in general, this second part sounds the same as the first.

After the second breakdown, **one final chord** is played to indicate that the song has come to an end. This is the typical way a Light Drum & Bass track ends.

***Keygram***

In the keygram we can see that the **intro is in the key of Dmaj or Dmin**. Usually the key is consistent in a Drum & Bass track, but the program finds it very hard to determine the key in the middle sections. It is possible that the many harmonies layered on top of each other make it very hard to determine one specific key. Additionally, this music is made with exclusively electronically generated sounds and **Spotify might have trouble analyzing** this when compared to actual chords played by a guitar.


### *Chromagram Comparison 2* - And for **Heavy Drum & Bass**? Comparing a **chromagram** to a **chordogram**

```{r out.width="50%"}

# Get track information
faceless <-
  get_tidy_audio_analysis("2m40HRbLhNf3FeH9ntqQWw") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)



# Make the chromagram. Use manhattan, euclidean, chebyshev
chroma_faceless <- faceless %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL , fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c() +
  labs(title = "Chromagram") +
   theme(plot.title = element_text(face = "bold",
                                   size = 14))

# Add lines and text at key moments
chroma_faceless +
  geom_vline(xintercept = 40,
             color = "white",
             size = 0.8) +
  geom_vline(xintercept = 110,
             color = "white",
             size = 0.8) + 
  geom_vline(xintercept = 133,
             color = "white",
             size = 0.8) +
  geom_vline(xintercept = 207,
             color = "white",
             size = 0.8) +
  annotate(geom="text", 
          x=40, 
          y="A", 
          label="First drop",
          color="white",
          fontface = 2,
          hjust=0) +
  annotate(geom="text", 
          x=110, 
          y="B", 
          label="Breakdown",
          color="white",
          fontface=2,
          hjust=1) +
  annotate(geom="text", 
          x=133, 
          y="A", 
          label="Second drop",
          color="white",
          fontface=2,
          hjust=0) +
  annotate(geom="text", 
          x=207, 
          y="B", 
          label="Breakdown",
          color="white",
          fontface=2,
          hjust=1)


# KEYGRAM
# Fetch analysis for a track
twenty_five <-
  get_tidy_audio_analysis("2m40HRbLhNf3FeH9ntqQWw") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

# Key or Chord 
twenty_five %>% 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "angular",    # Try different distance metrics
    norm = "euclidean"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "",
       title = "Chordogram") +
   theme(plot.title = element_text(face = "bold",
                                   size = 14)) +
  geom_vline(xintercept = 40,
             color = "white",
             size = 0.8) +
  geom_vline(xintercept = 110,
             color = "white",
             size = 0.8) + 
  geom_vline(xintercept = 133,
             color = "white",
             size = 0.8) +
  geom_vline(xintercept = 207,
             color = "white",
             size = 0.8)

```

***

*Track: [Faceless - Phace, Was A Be](https://open.spotify.com/track/2m40HRbLhNf3FeH9ntqQWw?si=5NqhOLJpSZiJIYiQmn1Atw)*

***Chromagram***

The intro of this song shows how a typical Heavy Drum & Bass track starts. **The drops are usually more intense**. This means that the build up to the drop starts early in the song. You can see this in the first part of the chromagram. The notes that are played (yellow) seem to have a staircase-like form. Playing a higher note each time will make the listener anticipate more and more to the drop. 

The middle part of tracks are unique in the sense that the underlying 'Drum & Bass' **drum pattern becomes harder to recognize**. Lots of deep basslines and samples with heavy electronic melodies are used in this part. Also, vocals are not used. This is why the chromagram looks so consistent.
Nothing interesting happens after the first breakdown. Just a few tones are played, and after a while a similar build up as before the first drop is used to anticipate towards the second drop.

The **ending of the track starts after the second breakdown**. Sometimes a Heavy Drum & Bass track ends instantly after the second breakdown. But in this case, a final sample (which still sounds pretty restless) is introduced prior to the breakdown. At some point the main samples stop, and the final sample is played for a short time before ending.

***Chordogram***

For reference, the white vertical lines are also placed in the chordogram. During the buildup towards the first drop, the chordogram is quite messy. You can see that something else is going on, but it is hard to determine what is happening. We can see that in general, the **chords used in the track stay consistent**, even during the intro (this is not visible in the chromagram). 



### *Chromagram Comparison 3* - Now, what is different in the build up of a typical **Jungle** song?

```{r}

# Get track information
good_enough <-
  get_tidy_audio_analysis("6tTn8EkWFZJHkuDXLI3Hzg") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

# Make the chromagram. Use manhattan, euclidean, chebyshev
chroma_good_enough <- good_enough %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Chromagram") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold",
                                   size = 14)) +
  scale_fill_viridis_c()

# Add lines and text at key moments
chroma_good_enough +
  geom_vline(xintercept = 45,
             color = "white",
             size = 0.8) +
  geom_vline(xintercept = 154,
             color = "white",
             size = 0.8) + 
  geom_vline(xintercept = 176,
             color = "white",
             size = 0.8) +
  geom_vline(xintercept = 265,
             color = "white",
             size = 0.8) +
  annotate(geom="text", 
          x=45, 
          y="A", 
          label="First drop",
          color="white",
          fontface = 2,
          hjust=0) +
  annotate(geom="text", 
          x=154, 
          y="B", 
          label="Breakdown",
          color="white",
          fontface=2,
          hjust=1) +
  annotate(geom="text", 
          x=176, 
          y="A", 
          label="Second drop",
          color="white",
          fontface=2,
          hjust=0) +
  annotate(geom="text", 
          x=265, 
          y="B", 
          label="Breakdown",
          color="white",
          fontface=2,
          hjust=1)

```

***

*Track: [Good Enough - Serial Killaz](https://open.spotify.com/track/6tTn8EkWFZJHkuDXLI3Hzg?si=eVMR2tO7Ta-UVtvT4_1_xQ)*

This track would be the average 'Jungle' track. A 'Drum & Bass' **drum pattern starts a few seconds after the track begins**. This is played until the first drop.

In this genre, **drops are usually introduced by vocals**. The vocal part begins, and a bassline is added so that the track sounds more 'complete'. The chromagram's pattern of the middle parts of this track seem different when compared to the other tracks discussed, because **vocals are included here**. The vocals and the samples have their own sound, this is why multiple notes show a high magnitude (yellow). 

It is common for Jungle tracks to **end the song by using the same samples as in the intro** of the track.



### *Track Breakdown* - Self-Similarity Matrices of a **Ragga Drum & Bass track**

```{r out.width= "50%"} 

test <-
  get_tidy_audio_analysis("5jeAriText11q771wlloPR") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

# CHROMA BASED SSM
compmus_long_distance(
  test %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  test %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  feature = pitches,
  method = "euclidean"
) %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Time (s)", 
       y = "Time (s)",
       title = "Chroma Features") +
  theme_minimal() +
  scale_fill_viridis_c(guide = NULL) +
  theme(plot.title = element_text(face = "bold",
                                   size = 14)) 

# TIMBRE BASED SSM
test %>%
  compmus_self_similarity(timbre, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "Time (s)", 
       y = "Time (s)",
       title = "Timbre Features") +
  theme(plot.title = element_text(face = "bold",
                                   size = 14))

```

***

A track from the Ragga Drum & Bass genre is chosen for the comparison of pitch-based and timbre-based features because tracks from this genre have not been discussed yet.

*Track: [No Diggity - Kursiva, Mooncat, Earth Beat Movement](https://open.spotify.com/track/5jeAriText11q771wlloPR?si=bsTOoXsKT9m4Pp1EOJVTHg)*

Interestingly, there are only small differences between the two self-similarity matrices. This track's **overall structure is very similar** to the structure of tracks that we have discussed. The intro is visible in the lower left corner. Usually, there are diagonal lines in the chroma-based matrix when there is repetition in a track, for example when a chorus is played multiple times. This is not the case for Drum & Bass tracks. **The repetition takes on a different form**, and therefore there are almost no diagonal lines present in the matrix. 

Small changes in texture can be seen in the timbre-based matrix, especially **at approximately 190 seconds**. This section of the track is after a breakdown and is therefore very quiet. Suddenly, a **short and loud electronic melody** is played, which causes the change of color in the timbre-based matrix. This is **not a change in pitch** and therefore the differences are not visible in the pitch-based matrix.



### The **overall average tempo** of Jungle and Drum & Bass music

```{r out.width="50%"}

temp_jungle <- get_tidy_audio_analysis("5UeLuozsBQbBj1MmB6joHz")

temp_jungle %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", 
       y = "Tempo (BPM)",
       title = "Tempogram") +
  theme_classic() +
  theme(plot.title = element_text(face = "bold",
                                   size = 14))

```

***

*Track: [Turn Down the Lights - Benny Page](https://open.spotify.com/track/5UeLuozsBQbBj1MmB6joHz?si=nLP_Qj3_RWCSgqE0we7q5w)*

The tempogram of this track represents the average tempo of the majority of Jungle and Drum & Bass tracks. We can see **three lines in the tempogram** at equal distance from each other, where the middle line is the thick yellow line at around 350 BPM. The **actual tempo** of the track is represented by the lowest line, which is around **170 BPM**. This shows that Jungle as well as Drum & Bass tracks have a very high tempo compared to other music genres. 

*(Note: the idea was to show a tempogram for the average track of each genre, but unfortunately this takes a considerable amount of time to load. Also, after viewing the tempograms individually, the averages are so close together that the tempograms do not differ that much. Therefore only one tempogram was chosen to represent the tempo of the corpus.)*



### Conclusion / Discussion (In construction)

In construction

***

Panel is in construction
